{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:28pt; line-height:30pt; font-weight:bold; text-align:center;\">Large Margin Deep Networks for Classification </div>\n",
    "\n",
    "The large margin principle has produced remarkable theoretical and empirical results for classification (SVM - Vapnik, 1995) and regression problems (SVR - Drucker et al., 1997).\n",
    "\n",
    "Desirable benefits of large margin classifiers include : \n",
    "- better generalization properties\n",
    "- robustness to input perturbations.\n",
    "  \n",
    "However, exact large margin algorithms are only suitable for **shallow models** where the margin has an analytical form (the l2 norm of the parameters). To overcome the limitations of classical margin approaches, G.F. Elsayed et al. designed a novel **loss function based on a first-order approximation of the margin**. \n",
    "\n",
    "This loss function is applicable to any network architecture (e.g., arbitrary depth, activation function, use of convolutions, residual networks), and complements existing general-purpose regularization techniques such as weight-decay, dropout and batch normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **I. Theoretical definition**\n",
    "\n",
    "#### **1. Large margin principle**\n",
    "*The following definitions have already been explored in the SVM course in AML.*\n",
    "\n",
    "Consider a classification problem with n classes.  \n",
    "Suppose we use a function $f_i: X → R$, for $i = 1,. . ., n$ that generates a prediction score for classifying the input vector $x \\in X$ to class $i$. The predicted label is decided by the class with maximal score, i.e. $i^∗ = \\operatorname{argmax}_i f_i(x)$.\n",
    "\n",
    "Define the decision boundary for each class pair {i, j} as:\n",
    "$$D_{i,j} = \\{x | f_i(x) = f_j (x)\\} \\;\\;\\;\\;(1) $$  \n",
    "\n",
    "Under this definition, the distance of a point x to the decision boundary $D_{i,j}$ is defined as the smallest distance to be moved so that x\n",
    "reaches the decision boundary, implying a score tie. Hence,\n",
    "$$d_{f,x,\\{i,j\\}} = \\min_δ \\|δ\\|_p \\; \\; s.t. \\; \\; f_i(x + δ) = f_j (x + δ) \\;\\;\\;\\;(2) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2. Large Margin Deep Networks**\n",
    "\n",
    "Using the above distance, we can develop a large margin loss:\n",
    "\n",
    "We start with a training set consisting of pairs $(x_k, y_k)$. We penalize the displacement of each $x_k$ to satisfy the margin constraint for separating class $y_k$ from class i ($i \\ne y_k$). This implies using the following loss function:\n",
    "$$ \\max\\{0, γ + d_{f,x_k,\\{i,y_k\\}} sign (f_i(x_k) − f_{yk}(x_k))\\} \\;\\;\\;\\;(3) $$\n",
    " \n",
    "In a multiclass setting, we aggregate individual losses arising from each $i \\ne y_k$ by some aggregation operator *A* ($max$ or $\\sum$):\n",
    "\n",
    "$$A_{i \\ne y_k} \\max\\{0, γ + d_{f,x_k,\\{i,y_k\\}} sign (f_i(x_k) − f_{yk}(x_k))\\}  \\;\\;\\;\\;(4) $$\n",
    "\n",
    "In order to learn $f_i$, we assume it is parameterized by a vector w and should use the notation $f_i(x; w)$; for brevity we keep using the notation $f_i(x)$. The goal is to minimize the loss w.r.t. w:\n",
    "$$w^∗  = \\operatorname{argmin}_w \\sum_k A_{i \\ne y_k} \\max\\{0, γ + d_{f,x_k,\\{i,y_k\\}} sign (f_i(x_k) − f_{yk}(x_k))\\} \\;\\;\\;\\;(5) $$\n",
    "\n",
    "The above formulation depends on d, whose exact computation from (2) is intractable when $f_i$’s are nonlinear (remember we are dealing with neural networks!). Instead, we present an **approximation to d**:\n",
    "$$ \\tilde{d}_{f,x,\\{i,j\\}} = \\frac{|f_i(x) − f_j (x)|}{\\| ∇_xf_i(x) - ∇_xf_j(x) \\|_q} \\;\\;(6)$$\n",
    "where $\\|\\|_q$ is the dual-norm of $\\|\\|_p$. Using the linear approximation, the loss function becomes after simplification:\n",
    "$$ \\hat{w} =  \\operatorname{argmin}_w \\sum_k A_{i \\ne y_k} \\max \\left\\{0, γ + \\frac{f_i(x_k) − f_{y_k} (x_k)}{\\| ∇_xf_i(x_k) - ∇_xf_{y_k}(x_k) \\|_q} \\right\\} \\;\\; (7)$$\n",
    "\n",
    "*NB.* (6) coincides with an SVM for the special case of a linear classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2. Margin for Hidden Layers**\n",
    "The classic notion of margin is defined based on the distance of input samples from the decision boundary using input/output association. In deep networks, however, the output is shaped from input by going through a number of transformations (layers). Thus, **we can define the margin based on any intermediate representation and the ultimate decision boundary**.\n",
    "\n",
    "The input x in the margin formulation (7) is replaced with the intermediate representation of x. More precisely, let $h_l$ denote the output of the l’th layer ($h_0 = x$) and $γ_l$ be the margin enforced for its corresponding representation. Then the margin loss (7) can be adapted as following :\n",
    "$$ \\hat{w} =  \\operatorname{argmin}_w \\sum_{l,k} A_{i \\ne y_k} \\max \\left\\{0, γ_l + \\frac{f_i(x_k) − f_{y_k} (x_k)}{\\| ∇_{h_l}f_i(x_k) - ∇_{h_l}f_{y_k}(x_k) \\|_q} \\right\\} \\;\\; (8)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **II. Experiments**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils import data\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "from torch.optim import Adam\n",
    "\n",
    "from LargeMarginLoss import LargeMarginLoss\n",
    "from test import test\n",
    "from train import train_ce, train_lm\n",
    "from network import Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1. Data loading**\n",
    "We use the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = data.DataLoader(\n",
    "        datasets.MNIST('./data', train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize((0.1307,), (0.3081,))\n",
    "                    ])),\n",
    "        batch_size=256, shuffle=True, drop_last=True)\n",
    "\n",
    "test_loader = data.DataLoader(\n",
    "        datasets.MNIST('./data', train=False,\n",
    "                        transform=transforms.Compose([\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize((0.1307,), (0.3081,))\n",
    "                    ])),\n",
    "        batch_size=2048, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2. Network training with the Large Margin loss.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 5.024226\n",
      "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.817788\n",
      "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.611039\n",
      "Test set: Accuracy: 9782/10000 (98%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.283682\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 1.473816\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 1.438349\n",
      "Test set: Accuracy: 9864/10000 (99%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.337239\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 1.795408\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 1.331209\n",
      "Test set: Accuracy: 9885/10000 (99%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.080955\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.082739\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.178787\n",
      "Test set: Accuracy: 9900/10000 (99%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.176652\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.890306\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.350686\n",
      "Test set: Accuracy: 9900/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lm = LargeMarginLoss(\n",
    "    gamma=5,\n",
    "    top_k=3,\n",
    "    dist_norm=np.inf\n",
    ")\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "net = Net().to(device)\n",
    "optim = Adam(net.parameters())\n",
    "for i in range(0, 5):\n",
    "    train_lm(net, train_loader, optim, i, lm, device)\n",
    "    test(net, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2. Network training with the Cross Entropy loss.**\n",
    "Comparing with Cross Entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.299823\n",
      "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.027824\n",
      "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.091731\n",
      "Test set: Accuracy: 9795/10000 (98%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.033656\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.057787\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.028668\n",
      "Test set: Accuracy: 9870/10000 (99%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.031245\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.028349\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.024944\n",
      "Test set: Accuracy: 9862/10000 (99%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.023172\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.018221\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.033606\n",
      "Test set: Accuracy: 9912/10000 (99%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.029208\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.018643\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.004380\n",
      "Test set: Accuracy: 9917/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net = Net().to(device)\n",
    "optim = Adam(net.parameters())\n",
    "for i in range(0, 5):    \n",
    "    train_ce(net, train_loader, optim, i, device)\n",
    "    test(net, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross Entropy and Large Margin loss seem to lead to similar accuracies.\n",
    "\n",
    "#### **4. Network training with SVM.**\n",
    "We compare results to the SVM results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "for (data, target) in train_loader:\n",
    "    X_train.append(torch.reshape(data,(256,-1)))\n",
    "    y_train.append(torch.reshape(target,(256,-1)))\n",
    "\n",
    "X_train = np.concatenate(X_train,axis=0)\n",
    "y_train = np.concatenate(y_train,axis=0)\n",
    "    \n",
    "X_test = []\n",
    "y_test = []\n",
    "for (data, target) in test_loader:\n",
    "    X_test.append(torch.reshape(data,(-1,784)))\n",
    "    y_test.append(torch.reshape(target,(-1,1)))\n",
    "\n",
    "X_test = np.concatenate(X_test,axis=0)\n",
    "y_test = np.concatenate(y_test,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1135"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = svm.SVC(kernel='rbf', gamma=10)\n",
    "clf.fit(X_train[:10000,:], y_train[:10000])\n",
    "\n",
    "y_test_pred = clf.predict(X_test)\n",
    "accuracy_score(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"><b>Things to keep in mind:</b>\n",
    "\n",
    "- **Decision Boundary**: Separation point between two different classes. At the decision boundary, there is ambiguity in class decisions.\n",
    "- **Margin**: The smallest non negative distance between decision boundary and closest class point\n",
    "- **Support Vector machines**: The most well known maximum margin principle based classification models - use support vectors (points closest to\n",
    "decision boundary) to estimate margin.\n",
    "- **Margins in Deep Networks**: Easy to compute in output space, very difficult (sometimes impossible) to compute in input space. The solution is a novel network-agnostic loss function which captures the principle of large margin separation in both the input and hidden layers for deep neural networks.\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cbb6105d5ece3a9d4cb8dea6d130a7c99b37976ba45b56ad2497505e8839ccd5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('sdd': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
